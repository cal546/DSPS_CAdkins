{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LongHW10_AutoEncoder.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mlBky2_VCiSX"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cal546/DSPS_CAdkins/blob/main/hw10/AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import Dense#, Dropout, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "#from keras.layers import Conv2D, MaxPooling2D\n",
        "#from keras import backend as K\n",
        "#import glob\n",
        "import pylab as pl\n",
        "from PIL import Image\n",
        "%pylab inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMp7Gl2NMzaZ",
        "outputId": "e7e3bccf-682f-4ca5-cf15-02112d08e955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8mW27gnf7U4"
      },
      "source": [
        "# 1. change kernel to GPU \n",
        "go to runtime -> change runtime type -> GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-jfXI90M0yE"
      },
      "source": [
        "### regression\n",
        "- loss='mean_squared_error' L2: default loss to use for regression problems. => linear activation function in output layer, one node out\n",
        "\n",
        "alternatives:  loss='mean_squared_logarithmic_error', 'mean_absolute_error' (which is L1 instead of L2)\n",
        "### binary classification\n",
        "\n",
        "- loss='binary_crossentropy' => sigmoid activation function in output layer, one node out\n",
        "\n",
        "alternatives: 'hinge'\n",
        "\n",
        "### multiclass classification\n",
        "categorical encoded as numerical\n",
        "- loss='categorical_crossentropy' => softmax n nodes out\n",
        "\n",
        "onehot encoded categoridal\n",
        "- 'parse_categorical_crossentropy' => softmax n nodes out\n",
        "\n",
        "- 'kullback Leibler Divergence Loss' => probabilistic categorical classification; log(P/Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZORHj6SPha0"
      },
      "source": [
        "## optimizers\n",
        "- SGD: stocastic gradient descent \n",
        "    - nesterov=True -> momentum inclusion\n",
        "- adam: Adaptive moment estimation. **good in most cases**\n",
        "- adagrad: different steps for different parameters based on frequency (binary input) well-suited for dealing with sparse data.\n",
        "\n",
        "- adaDelta: like adagrad but compensated for vanishing learning rate problem\n",
        "\n",
        "momentum refers to looking one step back and make a decision that includes the slope there\n",
        "\n",
        "### parameter:\n",
        "generally you need to adjust the learning rate which is how much you change the parameters by at each step. \n",
        "keras.optimizers.Adam(lr=0.001)\n",
        "\n",
        "\n",
        "https://gitcdn.xyz/cdn/Tony607/blog_statics/e1a0b1e060e783bd1978a141acff897ae71bd021/images/optimizer/optimizer.gif"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, l_train), (x_test, l_test) = mnist.load_data()\n",
        "\n",
        "y_train = np.array([np.zeros(10) for i in range(len(x_train))])\n",
        "y_test = np.array([np.zeros(10) for i in range(len(x_test))])\n",
        "for i in range(len(l_train)):\n",
        "  y_train[i][l_train[i]] = 1\n",
        "for i in range(len(l_test)):\n",
        "  y_test[i][l_test[i]] = 1\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "pl.imshow(x_train[0])\n",
        "pl.axis('off')\n",
        "intialshape = x_train[0].shape\n",
        "ndim = np.prod(x_train[0].shape)\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "aU0OCir8M6gf",
        "outputId": "94630dbe-beca-4045-fb33-e57cd5b1d294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q3auNVqxjgf",
        "outputId": "bbda67d2-7fb6-49cf-a729-971dbfd43fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = (x_train.astype(float) / 255).reshape(len(x_train), ndim)\n",
        "x_test = (x_test.astype(float) / 255).reshape(len(x_test), ndim)\n",
        "x_train.shape, x_test.shape\n",
        "xshape = x_train.shape[1]"
      ],
      "metadata": {
        "id": "nGXYKByNxlEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myCallback = EarlyStopping(monitor='val_loss', patience=5, min_delta=0.0001)"
      ],
      "metadata": {
        "id": "MasiyS2I40g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numberGuesser = Sequential()\n",
        "## encoder\n",
        "# input layer and the output size\n",
        "numberGuesser.add(Dense(128, activation='relu', input_dim=xshape))\n",
        "numberGuesser.add(Dense(64, activation='relu'))\n",
        "numberGuesser.add(Dense(32, activation='relu'))\n",
        "#bottle neck\n",
        "numberGuesser.add(Dense(16, activation='relu'))\n",
        "numberGuesser.add(Dense(10, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "5iKsY9KMyJ4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numberGuesser.compile(optimizer=\"adadelta\", loss = \"binary_crossentropy\")"
      ],
      "metadata": {
        "id": "-xu_lzsl2WjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numberGuesser.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVgt1RqVzbYn",
        "outputId": "e33e0083-f70f-4509-f909-69f586600711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                170       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 111,514\n",
            "Trainable params: 111,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numberFitter = numberGuesser.fit(x_train, y_train, validation_data=(x_test, y_test), epochs = 2000, batch_size=100, verbose =1, callbacks= [myCallback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TzFlfdPzpa8",
        "outputId": "a91391a6-d761-48a5-8479-bfa1e8874a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.6891 - val_loss: 0.6838\n",
            "Epoch 2/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.6784 - val_loss: 0.6721\n",
            "Epoch 3/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.6660 - val_loss: 0.6581\n",
            "Epoch 4/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.6498 - val_loss: 0.6387\n",
            "Epoch 5/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.6267 - val_loss: 0.6107\n",
            "Epoch 6/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.5929 - val_loss: 0.5698\n",
            "Epoch 7/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.5466 - val_loss: 0.5181\n",
            "Epoch 8/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.4928 - val_loss: 0.4627\n",
            "Epoch 9/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.4386 - val_loss: 0.4112\n",
            "Epoch 10/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3932 - val_loss: 0.3740\n",
            "Epoch 11/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3644 - val_loss: 0.3540\n",
            "Epoch 12/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3500 - val_loss: 0.3448\n",
            "Epoch 13/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.3431 - val_loss: 0.3402\n",
            "Epoch 14/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.3394 - val_loss: 0.3374\n",
            "Epoch 15/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3369 - val_loss: 0.3353\n",
            "Epoch 16/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.3348 - val_loss: 0.3334\n",
            "Epoch 17/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.3330 - val_loss: 0.3316\n",
            "Epoch 18/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.3312 - val_loss: 0.3299\n",
            "Epoch 19/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3295 - val_loss: 0.3283\n",
            "Epoch 20/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.3279 - val_loss: 0.3267\n",
            "Epoch 21/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3263 - val_loss: 0.3251\n",
            "Epoch 22/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.3248 - val_loss: 0.3236\n",
            "Epoch 23/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3232 - val_loss: 0.3220\n",
            "Epoch 24/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3217 - val_loss: 0.3205\n",
            "Epoch 25/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3202 - val_loss: 0.3190\n",
            "Epoch 26/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3186 - val_loss: 0.3174\n",
            "Epoch 27/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3171 - val_loss: 0.3159\n",
            "Epoch 28/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3156 - val_loss: 0.3143\n",
            "Epoch 29/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3141 - val_loss: 0.3128\n",
            "Epoch 30/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3125 - val_loss: 0.3112\n",
            "Epoch 31/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3110 - val_loss: 0.3097\n",
            "Epoch 32/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3094 - val_loss: 0.3081\n",
            "Epoch 33/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3079 - val_loss: 0.3065\n",
            "Epoch 34/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.3063 - val_loss: 0.3049\n",
            "Epoch 35/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3047 - val_loss: 0.3033\n",
            "Epoch 36/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3031 - val_loss: 0.3017\n",
            "Epoch 37/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.3015 - val_loss: 0.3000\n",
            "Epoch 38/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2999 - val_loss: 0.2983\n",
            "Epoch 39/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2982 - val_loss: 0.2967\n",
            "Epoch 40/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2966 - val_loss: 0.2950\n",
            "Epoch 41/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2949 - val_loss: 0.2932\n",
            "Epoch 42/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2932 - val_loss: 0.2915\n",
            "Epoch 43/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2915 - val_loss: 0.2898\n",
            "Epoch 44/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2898 - val_loss: 0.2880\n",
            "Epoch 45/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2880 - val_loss: 0.2862\n",
            "Epoch 46/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2862 - val_loss: 0.2843\n",
            "Epoch 47/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2844 - val_loss: 0.2824\n",
            "Epoch 48/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2825 - val_loss: 0.2804\n",
            "Epoch 49/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2805 - val_loss: 0.2784\n",
            "Epoch 50/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2784 - val_loss: 0.2763\n",
            "Epoch 51/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.2763 - val_loss: 0.2741\n",
            "Epoch 52/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2742 - val_loss: 0.2720\n",
            "Epoch 53/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.2722 - val_loss: 0.2700\n",
            "Epoch 54/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2702 - val_loss: 0.2680\n",
            "Epoch 55/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2682 - val_loss: 0.2660\n",
            "Epoch 56/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2663 - val_loss: 0.2640\n",
            "Epoch 57/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2643 - val_loss: 0.2621\n",
            "Epoch 58/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2624 - val_loss: 0.2601\n",
            "Epoch 59/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2604 - val_loss: 0.2582\n",
            "Epoch 60/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.2585 - val_loss: 0.2562\n",
            "Epoch 61/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2566 - val_loss: 0.2543\n",
            "Epoch 62/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2546 - val_loss: 0.2523\n",
            "Epoch 63/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2527 - val_loss: 0.2504\n",
            "Epoch 64/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2508 - val_loss: 0.2485\n",
            "Epoch 65/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2489 - val_loss: 0.2466\n",
            "Epoch 66/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2470 - val_loss: 0.2447\n",
            "Epoch 67/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2452 - val_loss: 0.2428\n",
            "Epoch 68/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2433 - val_loss: 0.2410\n",
            "Epoch 69/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2415 - val_loss: 0.2391\n",
            "Epoch 70/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2397 - val_loss: 0.2373\n",
            "Epoch 71/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2379 - val_loss: 0.2356\n",
            "Epoch 72/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2362 - val_loss: 0.2338\n",
            "Epoch 73/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2344 - val_loss: 0.2321\n",
            "Epoch 74/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.2327 - val_loss: 0.2303\n",
            "Epoch 75/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2310 - val_loss: 0.2286\n",
            "Epoch 76/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2293 - val_loss: 0.2269\n",
            "Epoch 77/2000\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 0.2276 - val_loss: 0.2252\n",
            "Epoch 78/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2259 - val_loss: 0.2235\n",
            "Epoch 79/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2243 - val_loss: 0.2218\n",
            "Epoch 80/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2226 - val_loss: 0.2201\n",
            "Epoch 81/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2209 - val_loss: 0.2184\n",
            "Epoch 82/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2192 - val_loss: 0.2167\n",
            "Epoch 83/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2176 - val_loss: 0.2150\n",
            "Epoch 84/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2159 - val_loss: 0.2133\n",
            "Epoch 85/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2142 - val_loss: 0.2117\n",
            "Epoch 86/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2126 - val_loss: 0.2100\n",
            "Epoch 87/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2110 - val_loss: 0.2084\n",
            "Epoch 88/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2094 - val_loss: 0.2068\n",
            "Epoch 89/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2078 - val_loss: 0.2052\n",
            "Epoch 90/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2063 - val_loss: 0.2037\n",
            "Epoch 91/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2048 - val_loss: 0.2022\n",
            "Epoch 92/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2033 - val_loss: 0.2007\n",
            "Epoch 93/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2019 - val_loss: 0.1993\n",
            "Epoch 94/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.2005 - val_loss: 0.1979\n",
            "Epoch 95/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1991 - val_loss: 0.1965\n",
            "Epoch 96/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1977 - val_loss: 0.1951\n",
            "Epoch 97/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1964 - val_loss: 0.1938\n",
            "Epoch 98/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1951 - val_loss: 0.1925\n",
            "Epoch 99/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1938 - val_loss: 0.1912\n",
            "Epoch 100/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1925 - val_loss: 0.1900\n",
            "Epoch 101/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1913 - val_loss: 0.1888\n",
            "Epoch 102/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1901 - val_loss: 0.1875\n",
            "Epoch 103/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1889 - val_loss: 0.1863\n",
            "Epoch 104/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1877 - val_loss: 0.1852\n",
            "Epoch 105/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1865 - val_loss: 0.1840\n",
            "Epoch 106/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1854 - val_loss: 0.1829\n",
            "Epoch 107/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1843 - val_loss: 0.1818\n",
            "Epoch 108/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1832 - val_loss: 0.1807\n",
            "Epoch 109/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1821 - val_loss: 0.1796\n",
            "Epoch 110/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1810 - val_loss: 0.1786\n",
            "Epoch 111/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1800 - val_loss: 0.1775\n",
            "Epoch 112/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1789 - val_loss: 0.1765\n",
            "Epoch 113/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1779 - val_loss: 0.1755\n",
            "Epoch 114/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1769 - val_loss: 0.1745\n",
            "Epoch 115/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1760 - val_loss: 0.1735\n",
            "Epoch 116/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1750 - val_loss: 0.1725\n",
            "Epoch 117/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1740 - val_loss: 0.1716\n",
            "Epoch 118/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1731 - val_loss: 0.1707\n",
            "Epoch 119/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1722 - val_loss: 0.1697\n",
            "Epoch 120/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1712 - val_loss: 0.1688\n",
            "Epoch 121/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1703 - val_loss: 0.1679\n",
            "Epoch 122/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1695 - val_loss: 0.1670\n",
            "Epoch 123/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1686 - val_loss: 0.1662\n",
            "Epoch 124/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1677 - val_loss: 0.1653\n",
            "Epoch 125/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1669 - val_loss: 0.1645\n",
            "Epoch 126/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1660 - val_loss: 0.1636\n",
            "Epoch 127/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1652 - val_loss: 0.1628\n",
            "Epoch 128/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1644 - val_loss: 0.1620\n",
            "Epoch 129/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.1612\n",
            "Epoch 130/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1628 - val_loss: 0.1604\n",
            "Epoch 131/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1620 - val_loss: 0.1596\n",
            "Epoch 132/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1612 - val_loss: 0.1589\n",
            "Epoch 133/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1605 - val_loss: 0.1581\n",
            "Epoch 134/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1597 - val_loss: 0.1573\n",
            "Epoch 135/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1590 - val_loss: 0.1566\n",
            "Epoch 136/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1583 - val_loss: 0.1559\n",
            "Epoch 137/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1575 - val_loss: 0.1552\n",
            "Epoch 138/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1568 - val_loss: 0.1545\n",
            "Epoch 139/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1561 - val_loss: 0.1538\n",
            "Epoch 140/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1554 - val_loss: 0.1531\n",
            "Epoch 141/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1548 - val_loss: 0.1524\n",
            "Epoch 142/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1541 - val_loss: 0.1517\n",
            "Epoch 143/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1534 - val_loss: 0.1511\n",
            "Epoch 144/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1528 - val_loss: 0.1504\n",
            "Epoch 145/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1521 - val_loss: 0.1498\n",
            "Epoch 146/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1515 - val_loss: 0.1491\n",
            "Epoch 147/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1508 - val_loss: 0.1485\n",
            "Epoch 148/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1502 - val_loss: 0.1478\n",
            "Epoch 149/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1496 - val_loss: 0.1472\n",
            "Epoch 150/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1490 - val_loss: 0.1466\n",
            "Epoch 151/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1484 - val_loss: 0.1460\n",
            "Epoch 152/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1478 - val_loss: 0.1454\n",
            "Epoch 153/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1472 - val_loss: 0.1448\n",
            "Epoch 154/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1466 - val_loss: 0.1442\n",
            "Epoch 155/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1460 - val_loss: 0.1436\n",
            "Epoch 156/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1454 - val_loss: 0.1430\n",
            "Epoch 157/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1448 - val_loss: 0.1425\n",
            "Epoch 158/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1443 - val_loss: 0.1419\n",
            "Epoch 159/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1437 - val_loss: 0.1413\n",
            "Epoch 160/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1432 - val_loss: 0.1408\n",
            "Epoch 161/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1426 - val_loss: 0.1402\n",
            "Epoch 162/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1421 - val_loss: 0.1396\n",
            "Epoch 163/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1415 - val_loss: 0.1391\n",
            "Epoch 164/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1410 - val_loss: 0.1386\n",
            "Epoch 165/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1404 - val_loss: 0.1380\n",
            "Epoch 166/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1399 - val_loss: 0.1375\n",
            "Epoch 167/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1394 - val_loss: 0.1370\n",
            "Epoch 168/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1389 - val_loss: 0.1364\n",
            "Epoch 169/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1383 - val_loss: 0.1359\n",
            "Epoch 170/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1378 - val_loss: 0.1354\n",
            "Epoch 171/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1373 - val_loss: 0.1349\n",
            "Epoch 172/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1368 - val_loss: 0.1343\n",
            "Epoch 173/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1363 - val_loss: 0.1338\n",
            "Epoch 174/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1358 - val_loss: 0.1333\n",
            "Epoch 175/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1353 - val_loss: 0.1328\n",
            "Epoch 176/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1348 - val_loss: 0.1323\n",
            "Epoch 177/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1343 - val_loss: 0.1318\n",
            "Epoch 178/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1338 - val_loss: 0.1313\n",
            "Epoch 179/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1333 - val_loss: 0.1308\n",
            "Epoch 180/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1329 - val_loss: 0.1303\n",
            "Epoch 181/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1324 - val_loss: 0.1299\n",
            "Epoch 182/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1319 - val_loss: 0.1294\n",
            "Epoch 183/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1314 - val_loss: 0.1289\n",
            "Epoch 184/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1310 - val_loss: 0.1284\n",
            "Epoch 185/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1305 - val_loss: 0.1279\n",
            "Epoch 186/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1300 - val_loss: 0.1275\n",
            "Epoch 187/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1296 - val_loss: 0.1270\n",
            "Epoch 188/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1291 - val_loss: 0.1265\n",
            "Epoch 189/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1286 - val_loss: 0.1261\n",
            "Epoch 190/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1282 - val_loss: 0.1256\n",
            "Epoch 191/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1277 - val_loss: 0.1251\n",
            "Epoch 192/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1273 - val_loss: 0.1247\n",
            "Epoch 193/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1268 - val_loss: 0.1242\n",
            "Epoch 194/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1264 - val_loss: 0.1238\n",
            "Epoch 195/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1259 - val_loss: 0.1233\n",
            "Epoch 196/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1255 - val_loss: 0.1229\n",
            "Epoch 197/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1251 - val_loss: 0.1224\n",
            "Epoch 198/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1246 - val_loss: 0.1220\n",
            "Epoch 199/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1242 - val_loss: 0.1215\n",
            "Epoch 200/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1238 - val_loss: 0.1211\n",
            "Epoch 201/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1234 - val_loss: 0.1207\n",
            "Epoch 202/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1229 - val_loss: 0.1202\n",
            "Epoch 203/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1225 - val_loss: 0.1198\n",
            "Epoch 204/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1221 - val_loss: 0.1194\n",
            "Epoch 205/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1217 - val_loss: 0.1189\n",
            "Epoch 206/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1213 - val_loss: 0.1185\n",
            "Epoch 207/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1208 - val_loss: 0.1181\n",
            "Epoch 208/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1204 - val_loss: 0.1177\n",
            "Epoch 209/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1200 - val_loss: 0.1173\n",
            "Epoch 210/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1196 - val_loss: 0.1168\n",
            "Epoch 211/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1192 - val_loss: 0.1164\n",
            "Epoch 212/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1188 - val_loss: 0.1160\n",
            "Epoch 213/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1184 - val_loss: 0.1156\n",
            "Epoch 214/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1180 - val_loss: 0.1152\n",
            "Epoch 215/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1176 - val_loss: 0.1148\n",
            "Epoch 216/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1172 - val_loss: 0.1144\n",
            "Epoch 217/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1168 - val_loss: 0.1140\n",
            "Epoch 218/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1164 - val_loss: 0.1136\n",
            "Epoch 219/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1160 - val_loss: 0.1132\n",
            "Epoch 220/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1157 - val_loss: 0.1128\n",
            "Epoch 221/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1153 - val_loss: 0.1124\n",
            "Epoch 222/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1149 - val_loss: 0.1120\n",
            "Epoch 223/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1145 - val_loss: 0.1117\n",
            "Epoch 224/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1141 - val_loss: 0.1113\n",
            "Epoch 225/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1138 - val_loss: 0.1109\n",
            "Epoch 226/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1134 - val_loss: 0.1105\n",
            "Epoch 227/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1130 - val_loss: 0.1101\n",
            "Epoch 228/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1126 - val_loss: 0.1098\n",
            "Epoch 229/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1123 - val_loss: 0.1094\n",
            "Epoch 230/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1119 - val_loss: 0.1090\n",
            "Epoch 231/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1115 - val_loss: 0.1086\n",
            "Epoch 232/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1112 - val_loss: 0.1083\n",
            "Epoch 233/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1108 - val_loss: 0.1079\n",
            "Epoch 234/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1104 - val_loss: 0.1075\n",
            "Epoch 235/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1101 - val_loss: 0.1072\n",
            "Epoch 236/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1097 - val_loss: 0.1068\n",
            "Epoch 237/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1094 - val_loss: 0.1064\n",
            "Epoch 238/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1090 - val_loss: 0.1061\n",
            "Epoch 239/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1087 - val_loss: 0.1057\n",
            "Epoch 240/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1083 - val_loss: 0.1054\n",
            "Epoch 241/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1080 - val_loss: 0.1050\n",
            "Epoch 242/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1076 - val_loss: 0.1047\n",
            "Epoch 243/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1073 - val_loss: 0.1043\n",
            "Epoch 244/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1069 - val_loss: 0.1040\n",
            "Epoch 245/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1066 - val_loss: 0.1036\n",
            "Epoch 246/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1063 - val_loss: 0.1033\n",
            "Epoch 247/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1059 - val_loss: 0.1029\n",
            "Epoch 248/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1056 - val_loss: 0.1026\n",
            "Epoch 249/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1052 - val_loss: 0.1023\n",
            "Epoch 250/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1049 - val_loss: 0.1019\n",
            "Epoch 251/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1046 - val_loss: 0.1016\n",
            "Epoch 252/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1042 - val_loss: 0.1013\n",
            "Epoch 253/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1039 - val_loss: 0.1009\n",
            "Epoch 254/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1036 - val_loss: 0.1006\n",
            "Epoch 255/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1033 - val_loss: 0.1003\n",
            "Epoch 256/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1029 - val_loss: 0.0999\n",
            "Epoch 257/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1026 - val_loss: 0.0996\n",
            "Epoch 258/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1023 - val_loss: 0.0993\n",
            "Epoch 259/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1020 - val_loss: 0.0990\n",
            "Epoch 260/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1017 - val_loss: 0.0987\n",
            "Epoch 261/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1013 - val_loss: 0.0983\n",
            "Epoch 262/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1010 - val_loss: 0.0980\n",
            "Epoch 263/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1007 - val_loss: 0.0977\n",
            "Epoch 264/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1004 - val_loss: 0.0974\n",
            "Epoch 265/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.1001 - val_loss: 0.0971\n",
            "Epoch 266/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0998 - val_loss: 0.0968\n",
            "Epoch 267/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0995 - val_loss: 0.0965\n",
            "Epoch 268/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0992 - val_loss: 0.0962\n",
            "Epoch 269/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0989 - val_loss: 0.0959\n",
            "Epoch 270/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0986 - val_loss: 0.0956\n",
            "Epoch 271/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0983 - val_loss: 0.0953\n",
            "Epoch 272/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0980 - val_loss: 0.0950\n",
            "Epoch 273/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0977 - val_loss: 0.0947\n",
            "Epoch 274/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0974 - val_loss: 0.0944\n",
            "Epoch 275/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0971 - val_loss: 0.0941\n",
            "Epoch 276/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0968 - val_loss: 0.0938\n",
            "Epoch 277/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0965 - val_loss: 0.0935\n",
            "Epoch 278/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0963 - val_loss: 0.0932\n",
            "Epoch 279/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0960 - val_loss: 0.0929\n",
            "Epoch 280/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0957 - val_loss: 0.0927\n",
            "Epoch 281/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0954 - val_loss: 0.0924\n",
            "Epoch 282/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0951 - val_loss: 0.0921\n",
            "Epoch 283/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0949 - val_loss: 0.0918\n",
            "Epoch 284/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0946 - val_loss: 0.0915\n",
            "Epoch 285/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0943 - val_loss: 0.0913\n",
            "Epoch 286/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0940 - val_loss: 0.0910\n",
            "Epoch 287/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0938 - val_loss: 0.0907\n",
            "Epoch 288/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0935 - val_loss: 0.0905\n",
            "Epoch 289/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0932 - val_loss: 0.0902\n",
            "Epoch 290/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0930 - val_loss: 0.0899\n",
            "Epoch 291/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0927 - val_loss: 0.0897\n",
            "Epoch 292/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0924 - val_loss: 0.0894\n",
            "Epoch 293/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0922 - val_loss: 0.0892\n",
            "Epoch 294/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0919 - val_loss: 0.0889\n",
            "Epoch 295/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0917 - val_loss: 0.0887\n",
            "Epoch 296/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0914 - val_loss: 0.0884\n",
            "Epoch 297/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0912 - val_loss: 0.0882\n",
            "Epoch 298/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0909 - val_loss: 0.0879\n",
            "Epoch 299/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0907 - val_loss: 0.0877\n",
            "Epoch 300/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0904 - val_loss: 0.0874\n",
            "Epoch 301/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0902 - val_loss: 0.0872\n",
            "Epoch 302/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0899 - val_loss: 0.0869\n",
            "Epoch 303/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0897 - val_loss: 0.0867\n",
            "Epoch 304/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0895 - val_loss: 0.0865\n",
            "Epoch 305/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0892 - val_loss: 0.0862\n",
            "Epoch 306/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0890 - val_loss: 0.0860\n",
            "Epoch 307/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0888 - val_loss: 0.0858\n",
            "Epoch 308/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0885 - val_loss: 0.0855\n",
            "Epoch 309/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0883 - val_loss: 0.0853\n",
            "Epoch 310/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0881 - val_loss: 0.0851\n",
            "Epoch 311/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0878 - val_loss: 0.0848\n",
            "Epoch 312/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0876 - val_loss: 0.0846\n",
            "Epoch 313/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0874 - val_loss: 0.0844\n",
            "Epoch 314/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0872 - val_loss: 0.0842\n",
            "Epoch 315/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0870 - val_loss: 0.0840\n",
            "Epoch 316/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0867 - val_loss: 0.0837\n",
            "Epoch 317/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0865 - val_loss: 0.0835\n",
            "Epoch 318/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0863 - val_loss: 0.0833\n",
            "Epoch 319/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0861 - val_loss: 0.0831\n",
            "Epoch 320/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0859 - val_loss: 0.0829\n",
            "Epoch 321/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0857 - val_loss: 0.0827\n",
            "Epoch 322/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0855 - val_loss: 0.0825\n",
            "Epoch 323/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0852 - val_loss: 0.0823\n",
            "Epoch 324/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0850 - val_loss: 0.0821\n",
            "Epoch 325/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0848 - val_loss: 0.0819\n",
            "Epoch 326/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0846 - val_loss: 0.0817\n",
            "Epoch 327/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0844 - val_loss: 0.0815\n",
            "Epoch 328/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0842 - val_loss: 0.0813\n",
            "Epoch 329/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0840 - val_loss: 0.0811\n",
            "Epoch 330/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0838 - val_loss: 0.0809\n",
            "Epoch 331/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0836 - val_loss: 0.0807\n",
            "Epoch 332/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0834 - val_loss: 0.0805\n",
            "Epoch 333/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0832 - val_loss: 0.0803\n",
            "Epoch 334/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0830 - val_loss: 0.0801\n",
            "Epoch 335/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0829 - val_loss: 0.0799\n",
            "Epoch 336/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0827 - val_loss: 0.0797\n",
            "Epoch 337/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0825 - val_loss: 0.0796\n",
            "Epoch 338/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0823 - val_loss: 0.0794\n",
            "Epoch 339/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0821 - val_loss: 0.0792\n",
            "Epoch 340/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0819 - val_loss: 0.0790\n",
            "Epoch 341/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0817 - val_loss: 0.0788\n",
            "Epoch 342/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0816 - val_loss: 0.0787\n",
            "Epoch 343/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0814 - val_loss: 0.0785\n",
            "Epoch 344/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0812 - val_loss: 0.0783\n",
            "Epoch 345/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0810 - val_loss: 0.0781\n",
            "Epoch 346/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0808 - val_loss: 0.0780\n",
            "Epoch 347/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0807 - val_loss: 0.0778\n",
            "Epoch 348/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0805 - val_loss: 0.0776\n",
            "Epoch 349/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0803 - val_loss: 0.0775\n",
            "Epoch 350/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0801 - val_loss: 0.0773\n",
            "Epoch 351/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0800 - val_loss: 0.0771\n",
            "Epoch 352/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0798 - val_loss: 0.0770\n",
            "Epoch 353/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0796 - val_loss: 0.0768\n",
            "Epoch 354/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0795 - val_loss: 0.0766\n",
            "Epoch 355/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0793 - val_loss: 0.0765\n",
            "Epoch 356/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0791 - val_loss: 0.0763\n",
            "Epoch 357/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0790 - val_loss: 0.0761\n",
            "Epoch 358/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0788 - val_loss: 0.0760\n",
            "Epoch 359/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0786 - val_loss: 0.0758\n",
            "Epoch 360/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0785 - val_loss: 0.0757\n",
            "Epoch 361/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0783 - val_loss: 0.0755\n",
            "Epoch 362/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0782 - val_loss: 0.0754\n",
            "Epoch 363/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0780 - val_loss: 0.0752\n",
            "Epoch 364/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0779 - val_loss: 0.0751\n",
            "Epoch 365/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0777 - val_loss: 0.0749\n",
            "Epoch 366/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0776 - val_loss: 0.0748\n",
            "Epoch 367/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0774 - val_loss: 0.0746\n",
            "Epoch 368/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0773 - val_loss: 0.0745\n",
            "Epoch 369/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0771 - val_loss: 0.0743\n",
            "Epoch 370/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0770 - val_loss: 0.0742\n",
            "Epoch 371/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0768 - val_loss: 0.0740\n",
            "Epoch 372/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0767 - val_loss: 0.0739\n",
            "Epoch 373/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0765 - val_loss: 0.0737\n",
            "Epoch 374/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0764 - val_loss: 0.0736\n",
            "Epoch 375/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0762 - val_loss: 0.0735\n",
            "Epoch 376/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0761 - val_loss: 0.0733\n",
            "Epoch 377/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0759 - val_loss: 0.0732\n",
            "Epoch 378/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0758 - val_loss: 0.0731\n",
            "Epoch 379/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0756 - val_loss: 0.0729\n",
            "Epoch 380/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0755 - val_loss: 0.0728\n",
            "Epoch 381/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0754 - val_loss: 0.0727\n",
            "Epoch 382/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0752 - val_loss: 0.0725\n",
            "Epoch 383/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0751 - val_loss: 0.0724\n",
            "Epoch 384/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0750 - val_loss: 0.0723\n",
            "Epoch 385/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0748 - val_loss: 0.0721\n",
            "Epoch 386/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0747 - val_loss: 0.0720\n",
            "Epoch 387/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0746 - val_loss: 0.0719\n",
            "Epoch 388/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0744 - val_loss: 0.0717\n",
            "Epoch 389/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0743 - val_loss: 0.0716\n",
            "Epoch 390/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0742 - val_loss: 0.0715\n",
            "Epoch 391/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0740 - val_loss: 0.0714\n",
            "Epoch 392/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0739 - val_loss: 0.0712\n",
            "Epoch 393/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0738 - val_loss: 0.0711\n",
            "Epoch 394/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0736 - val_loss: 0.0710\n",
            "Epoch 395/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0735 - val_loss: 0.0709\n",
            "Epoch 396/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0734 - val_loss: 0.0707\n",
            "Epoch 397/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0733 - val_loss: 0.0706\n",
            "Epoch 398/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0731 - val_loss: 0.0705\n",
            "Epoch 399/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0730 - val_loss: 0.0704\n",
            "Epoch 400/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0729 - val_loss: 0.0703\n",
            "Epoch 401/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0728 - val_loss: 0.0702\n",
            "Epoch 402/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0726 - val_loss: 0.0700\n",
            "Epoch 403/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0725 - val_loss: 0.0699\n",
            "Epoch 404/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0724 - val_loss: 0.0698\n",
            "Epoch 405/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0723 - val_loss: 0.0697\n",
            "Epoch 406/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0722 - val_loss: 0.0696\n",
            "Epoch 407/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0720 - val_loss: 0.0695\n",
            "Epoch 408/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0719 - val_loss: 0.0693\n",
            "Epoch 409/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0718 - val_loss: 0.0692\n",
            "Epoch 410/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0717 - val_loss: 0.0691\n",
            "Epoch 411/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0716 - val_loss: 0.0690\n",
            "Epoch 412/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0715 - val_loss: 0.0689\n",
            "Epoch 413/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0713 - val_loss: 0.0688\n",
            "Epoch 414/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0712 - val_loss: 0.0687\n",
            "Epoch 415/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0711 - val_loss: 0.0686\n",
            "Epoch 416/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0710 - val_loss: 0.0685\n",
            "Epoch 417/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0709 - val_loss: 0.0684\n",
            "Epoch 418/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0708 - val_loss: 0.0683\n",
            "Epoch 419/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0707 - val_loss: 0.0682\n",
            "Epoch 420/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0706 - val_loss: 0.0681\n",
            "Epoch 421/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0705 - val_loss: 0.0680\n",
            "Epoch 422/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0704 - val_loss: 0.0679\n",
            "Epoch 423/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0702 - val_loss: 0.0678\n",
            "Epoch 424/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0701 - val_loss: 0.0677\n",
            "Epoch 425/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0700 - val_loss: 0.0676\n",
            "Epoch 426/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0699 - val_loss: 0.0675\n",
            "Epoch 427/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0698 - val_loss: 0.0674\n",
            "Epoch 428/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0697 - val_loss: 0.0673\n",
            "Epoch 429/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0696 - val_loss: 0.0672\n",
            "Epoch 430/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0695 - val_loss: 0.0671\n",
            "Epoch 431/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0694 - val_loss: 0.0670\n",
            "Epoch 432/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0693 - val_loss: 0.0669\n",
            "Epoch 433/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0692 - val_loss: 0.0668\n",
            "Epoch 434/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0691 - val_loss: 0.0667\n",
            "Epoch 435/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0690 - val_loss: 0.0666\n",
            "Epoch 436/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0689 - val_loss: 0.0665\n",
            "Epoch 437/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0688 - val_loss: 0.0664\n",
            "Epoch 438/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0687 - val_loss: 0.0663\n",
            "Epoch 439/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0686 - val_loss: 0.0662\n",
            "Epoch 440/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0685 - val_loss: 0.0661\n",
            "Epoch 441/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0684 - val_loss: 0.0660\n",
            "Epoch 442/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0683 - val_loss: 0.0659\n",
            "Epoch 443/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0682 - val_loss: 0.0659\n",
            "Epoch 444/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0681 - val_loss: 0.0658\n",
            "Epoch 445/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0680 - val_loss: 0.0657\n",
            "Epoch 446/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0679 - val_loss: 0.0656\n",
            "Epoch 447/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0678 - val_loss: 0.0655\n",
            "Epoch 448/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0677 - val_loss: 0.0654\n",
            "Epoch 449/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0676 - val_loss: 0.0653\n",
            "Epoch 450/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0675 - val_loss: 0.0652\n",
            "Epoch 451/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0675 - val_loss: 0.0652\n",
            "Epoch 452/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0674 - val_loss: 0.0651\n",
            "Epoch 453/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0673 - val_loss: 0.0650\n",
            "Epoch 454/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0672 - val_loss: 0.0649\n",
            "Epoch 455/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0671 - val_loss: 0.0648\n",
            "Epoch 456/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0670 - val_loss: 0.0647\n",
            "Epoch 457/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0669 - val_loss: 0.0646\n",
            "Epoch 458/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0668 - val_loss: 0.0646\n",
            "Epoch 459/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0667 - val_loss: 0.0645\n",
            "Epoch 460/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0666 - val_loss: 0.0644\n",
            "Epoch 461/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0666 - val_loss: 0.0643\n",
            "Epoch 462/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0665 - val_loss: 0.0642\n",
            "Epoch 463/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0664 - val_loss: 0.0642\n",
            "Epoch 464/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0663 - val_loss: 0.0641\n",
            "Epoch 465/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0662 - val_loss: 0.0640\n",
            "Epoch 466/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0661 - val_loss: 0.0639\n",
            "Epoch 467/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0660 - val_loss: 0.0638\n",
            "Epoch 468/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0659 - val_loss: 0.0638\n",
            "Epoch 469/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0659 - val_loss: 0.0637\n",
            "Epoch 470/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0658 - val_loss: 0.0636\n",
            "Epoch 471/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0657 - val_loss: 0.0635\n",
            "Epoch 472/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0656 - val_loss: 0.0634\n",
            "Epoch 473/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0655 - val_loss: 0.0634\n",
            "Epoch 474/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0654 - val_loss: 0.0633\n",
            "Epoch 475/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0654 - val_loss: 0.0632\n",
            "Epoch 476/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0653 - val_loss: 0.0631\n",
            "Epoch 477/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0652 - val_loss: 0.0631\n",
            "Epoch 478/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0651 - val_loss: 0.0630\n",
            "Epoch 479/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0650 - val_loss: 0.0629\n",
            "Epoch 480/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0650 - val_loss: 0.0628\n",
            "Epoch 481/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0649 - val_loss: 0.0628\n",
            "Epoch 482/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0648 - val_loss: 0.0627\n",
            "Epoch 483/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0647 - val_loss: 0.0626\n",
            "Epoch 484/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0646 - val_loss: 0.0625\n",
            "Epoch 485/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0646 - val_loss: 0.0625\n",
            "Epoch 486/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0645 - val_loss: 0.0624\n",
            "Epoch 487/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0644 - val_loss: 0.0623\n",
            "Epoch 488/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0643 - val_loss: 0.0622\n",
            "Epoch 489/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0642 - val_loss: 0.0622\n",
            "Epoch 490/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0642 - val_loss: 0.0621\n",
            "Epoch 491/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0641 - val_loss: 0.0620\n",
            "Epoch 492/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0640 - val_loss: 0.0620\n",
            "Epoch 493/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0639 - val_loss: 0.0619\n",
            "Epoch 494/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0639 - val_loss: 0.0618\n",
            "Epoch 495/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0638 - val_loss: 0.0617\n",
            "Epoch 496/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0637 - val_loss: 0.0617\n",
            "Epoch 497/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0636 - val_loss: 0.0616\n",
            "Epoch 498/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0636 - val_loss: 0.0615\n",
            "Epoch 499/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0635 - val_loss: 0.0615\n",
            "Epoch 500/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0634 - val_loss: 0.0614\n",
            "Epoch 501/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0633 - val_loss: 0.0613\n",
            "Epoch 502/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0633 - val_loss: 0.0613\n",
            "Epoch 503/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0632 - val_loss: 0.0612\n",
            "Epoch 504/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0631 - val_loss: 0.0611\n",
            "Epoch 505/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0630 - val_loss: 0.0611\n",
            "Epoch 506/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0630 - val_loss: 0.0610\n",
            "Epoch 507/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0629 - val_loss: 0.0609\n",
            "Epoch 508/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0628 - val_loss: 0.0609\n",
            "Epoch 509/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0628 - val_loss: 0.0608\n",
            "Epoch 510/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0627 - val_loss: 0.0607\n",
            "Epoch 511/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0626 - val_loss: 0.0607\n",
            "Epoch 512/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0625 - val_loss: 0.0606\n",
            "Epoch 513/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0625 - val_loss: 0.0605\n",
            "Epoch 514/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0624 - val_loss: 0.0605\n",
            "Epoch 515/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0623 - val_loss: 0.0604\n",
            "Epoch 516/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0623 - val_loss: 0.0603\n",
            "Epoch 517/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0622 - val_loss: 0.0603\n",
            "Epoch 518/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0621 - val_loss: 0.0602\n",
            "Epoch 519/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0621 - val_loss: 0.0602\n",
            "Epoch 520/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0620 - val_loss: 0.0601\n",
            "Epoch 521/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0619 - val_loss: 0.0600\n",
            "Epoch 522/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0619 - val_loss: 0.0600\n",
            "Epoch 523/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0618 - val_loss: 0.0599\n",
            "Epoch 524/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0617 - val_loss: 0.0598\n",
            "Epoch 525/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0617 - val_loss: 0.0598\n",
            "Epoch 526/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0616 - val_loss: 0.0597\n",
            "Epoch 527/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0615 - val_loss: 0.0597\n",
            "Epoch 528/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0615 - val_loss: 0.0596\n",
            "Epoch 529/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0614 - val_loss: 0.0595\n",
            "Epoch 530/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0613 - val_loss: 0.0595\n",
            "Epoch 531/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0613 - val_loss: 0.0594\n",
            "Epoch 532/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0612 - val_loss: 0.0594\n",
            "Epoch 533/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0611 - val_loss: 0.0593\n",
            "Epoch 534/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0611 - val_loss: 0.0592\n",
            "Epoch 535/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0610 - val_loss: 0.0592\n",
            "Epoch 536/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0609 - val_loss: 0.0591\n",
            "Epoch 537/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0609 - val_loss: 0.0591\n",
            "Epoch 538/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0608 - val_loss: 0.0590\n",
            "Epoch 539/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0607 - val_loss: 0.0589\n",
            "Epoch 540/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0607 - val_loss: 0.0589\n",
            "Epoch 541/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0606 - val_loss: 0.0588\n",
            "Epoch 542/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0606 - val_loss: 0.0588\n",
            "Epoch 543/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0605 - val_loss: 0.0587\n",
            "Epoch 544/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0604 - val_loss: 0.0586\n",
            "Epoch 545/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0604 - val_loss: 0.0586\n",
            "Epoch 546/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0603 - val_loss: 0.0585\n",
            "Epoch 547/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0602 - val_loss: 0.0585\n",
            "Epoch 548/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0602 - val_loss: 0.0584\n",
            "Epoch 549/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0601 - val_loss: 0.0584\n",
            "Epoch 550/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0601 - val_loss: 0.0583\n",
            "Epoch 551/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0600 - val_loss: 0.0582\n",
            "Epoch 552/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0599 - val_loss: 0.0582\n",
            "Epoch 553/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0599 - val_loss: 0.0581\n",
            "Epoch 554/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0598 - val_loss: 0.0581\n",
            "Epoch 555/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0597 - val_loss: 0.0580\n",
            "Epoch 556/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0597 - val_loss: 0.0580\n",
            "Epoch 557/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0596 - val_loss: 0.0579\n",
            "Epoch 558/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0596 - val_loss: 0.0579\n",
            "Epoch 559/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0595 - val_loss: 0.0578\n",
            "Epoch 560/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0595 - val_loss: 0.0577\n",
            "Epoch 561/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0594 - val_loss: 0.0577\n",
            "Epoch 562/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0593 - val_loss: 0.0576\n",
            "Epoch 563/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0593 - val_loss: 0.0576\n",
            "Epoch 564/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0592 - val_loss: 0.0575\n",
            "Epoch 565/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0592 - val_loss: 0.0575\n",
            "Epoch 566/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0591 - val_loss: 0.0574\n",
            "Epoch 567/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0590 - val_loss: 0.0574\n",
            "Epoch 568/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0590 - val_loss: 0.0573\n",
            "Epoch 569/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0589 - val_loss: 0.0573\n",
            "Epoch 570/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0589 - val_loss: 0.0572\n",
            "Epoch 571/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0588 - val_loss: 0.0572\n",
            "Epoch 572/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0588 - val_loss: 0.0571\n",
            "Epoch 573/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0587 - val_loss: 0.0570\n",
            "Epoch 574/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0586 - val_loss: 0.0570\n",
            "Epoch 575/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0586 - val_loss: 0.0569\n",
            "Epoch 576/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0585 - val_loss: 0.0569\n",
            "Epoch 577/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0585 - val_loss: 0.0568\n",
            "Epoch 578/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0584 - val_loss: 0.0568\n",
            "Epoch 579/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0584 - val_loss: 0.0567\n",
            "Epoch 580/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0583 - val_loss: 0.0567\n",
            "Epoch 581/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0582 - val_loss: 0.0566\n",
            "Epoch 582/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0582 - val_loss: 0.0566\n",
            "Epoch 583/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0581 - val_loss: 0.0565\n",
            "Epoch 584/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0581 - val_loss: 0.0565\n",
            "Epoch 585/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0580 - val_loss: 0.0564\n",
            "Epoch 586/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0580 - val_loss: 0.0564\n",
            "Epoch 587/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0579 - val_loss: 0.0563\n",
            "Epoch 588/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0579 - val_loss: 0.0563\n",
            "Epoch 589/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0578 - val_loss: 0.0562\n",
            "Epoch 590/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0577 - val_loss: 0.0562\n",
            "Epoch 591/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0577 - val_loss: 0.0561\n",
            "Epoch 592/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0576 - val_loss: 0.0561\n",
            "Epoch 593/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0576 - val_loss: 0.0560\n",
            "Epoch 594/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0575 - val_loss: 0.0560\n",
            "Epoch 595/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0575 - val_loss: 0.0559\n",
            "Epoch 596/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0574 - val_loss: 0.0559\n",
            "Epoch 597/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0574 - val_loss: 0.0558\n",
            "Epoch 598/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0573 - val_loss: 0.0558\n",
            "Epoch 599/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0573 - val_loss: 0.0557\n",
            "Epoch 600/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0572 - val_loss: 0.0557\n",
            "Epoch 601/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0572 - val_loss: 0.0556\n",
            "Epoch 602/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0571 - val_loss: 0.0556\n",
            "Epoch 603/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0571 - val_loss: 0.0555\n",
            "Epoch 604/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0570 - val_loss: 0.0555\n",
            "Epoch 605/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0570 - val_loss: 0.0555\n",
            "Epoch 606/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0569 - val_loss: 0.0554\n",
            "Epoch 607/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0568 - val_loss: 0.0554\n",
            "Epoch 608/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0568 - val_loss: 0.0553\n",
            "Epoch 609/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0567 - val_loss: 0.0553\n",
            "Epoch 610/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0567 - val_loss: 0.0552\n",
            "Epoch 611/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0566 - val_loss: 0.0552\n",
            "Epoch 612/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0566 - val_loss: 0.0551\n",
            "Epoch 613/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0565 - val_loss: 0.0551\n",
            "Epoch 614/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0565 - val_loss: 0.0550\n",
            "Epoch 615/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0564 - val_loss: 0.0550\n",
            "Epoch 616/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0564 - val_loss: 0.0549\n",
            "Epoch 617/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0563 - val_loss: 0.0549\n",
            "Epoch 618/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0563 - val_loss: 0.0548\n",
            "Epoch 619/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0562 - val_loss: 0.0548\n",
            "Epoch 620/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0562 - val_loss: 0.0548\n",
            "Epoch 621/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0561 - val_loss: 0.0547\n",
            "Epoch 622/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0561 - val_loss: 0.0547\n",
            "Epoch 623/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0560 - val_loss: 0.0546\n",
            "Epoch 624/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0560 - val_loss: 0.0546\n",
            "Epoch 625/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0559 - val_loss: 0.0545\n",
            "Epoch 626/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0559 - val_loss: 0.0545\n",
            "Epoch 627/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0558 - val_loss: 0.0544\n",
            "Epoch 628/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0558 - val_loss: 0.0544\n",
            "Epoch 629/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0557 - val_loss: 0.0543\n",
            "Epoch 630/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0557 - val_loss: 0.0543\n",
            "Epoch 631/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0556 - val_loss: 0.0543\n",
            "Epoch 632/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0556 - val_loss: 0.0542\n",
            "Epoch 633/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0555 - val_loss: 0.0542\n",
            "Epoch 634/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0555 - val_loss: 0.0541\n",
            "Epoch 635/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0555 - val_loss: 0.0541\n",
            "Epoch 636/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0554 - val_loss: 0.0541\n",
            "Epoch 637/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0554 - val_loss: 0.0540\n",
            "Epoch 638/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0553 - val_loss: 0.0540\n",
            "Epoch 639/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0553 - val_loss: 0.0539\n",
            "Epoch 640/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0552 - val_loss: 0.0539\n",
            "Epoch 641/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0552 - val_loss: 0.0538\n",
            "Epoch 642/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0551 - val_loss: 0.0538\n",
            "Epoch 643/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0551 - val_loss: 0.0537\n",
            "Epoch 644/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0550 - val_loss: 0.0537\n",
            "Epoch 645/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0550 - val_loss: 0.0537\n",
            "Epoch 646/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0549 - val_loss: 0.0536\n",
            "Epoch 647/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0549 - val_loss: 0.0536\n",
            "Epoch 648/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0548 - val_loss: 0.0535\n",
            "Epoch 649/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0548 - val_loss: 0.0535\n",
            "Epoch 650/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0547 - val_loss: 0.0534\n",
            "Epoch 651/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0547 - val_loss: 0.0534\n",
            "Epoch 652/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0547 - val_loss: 0.0534\n",
            "Epoch 653/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0546 - val_loss: 0.0533\n",
            "Epoch 654/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0546 - val_loss: 0.0533\n",
            "Epoch 655/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0545 - val_loss: 0.0532\n",
            "Epoch 656/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0545 - val_loss: 0.0532\n",
            "Epoch 657/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0544 - val_loss: 0.0532\n",
            "Epoch 658/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0544 - val_loss: 0.0531\n",
            "Epoch 659/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0543 - val_loss: 0.0531\n",
            "Epoch 660/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0543 - val_loss: 0.0530\n",
            "Epoch 661/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0542 - val_loss: 0.0530\n",
            "Epoch 662/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0542 - val_loss: 0.0530\n",
            "Epoch 663/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0541 - val_loss: 0.0529\n",
            "Epoch 664/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0541 - val_loss: 0.0529\n",
            "Epoch 665/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0541 - val_loss: 0.0528\n",
            "Epoch 666/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0540 - val_loss: 0.0528\n",
            "Epoch 667/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0540 - val_loss: 0.0528\n",
            "Epoch 668/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0539 - val_loss: 0.0527\n",
            "Epoch 669/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0539 - val_loss: 0.0527\n",
            "Epoch 670/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0538 - val_loss: 0.0526\n",
            "Epoch 671/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0538 - val_loss: 0.0526\n",
            "Epoch 672/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0538 - val_loss: 0.0526\n",
            "Epoch 673/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0537 - val_loss: 0.0525\n",
            "Epoch 674/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0537 - val_loss: 0.0525\n",
            "Epoch 675/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0536 - val_loss: 0.0524\n",
            "Epoch 676/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0536 - val_loss: 0.0524\n",
            "Epoch 677/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0535 - val_loss: 0.0524\n",
            "Epoch 678/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0535 - val_loss: 0.0523\n",
            "Epoch 679/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0534 - val_loss: 0.0523\n",
            "Epoch 680/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0534 - val_loss: 0.0522\n",
            "Epoch 681/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0534 - val_loss: 0.0522\n",
            "Epoch 682/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0533 - val_loss: 0.0522\n",
            "Epoch 683/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0533 - val_loss: 0.0521\n",
            "Epoch 684/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0532 - val_loss: 0.0521\n",
            "Epoch 685/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0532 - val_loss: 0.0520\n",
            "Epoch 686/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0531 - val_loss: 0.0520\n",
            "Epoch 687/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0531 - val_loss: 0.0520\n",
            "Epoch 688/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0531 - val_loss: 0.0519\n",
            "Epoch 689/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0530 - val_loss: 0.0519\n",
            "Epoch 690/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0530 - val_loss: 0.0518\n",
            "Epoch 691/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0529 - val_loss: 0.0518\n",
            "Epoch 692/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0529 - val_loss: 0.0518\n",
            "Epoch 693/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0528 - val_loss: 0.0517\n",
            "Epoch 694/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0528 - val_loss: 0.0517\n",
            "Epoch 695/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0528 - val_loss: 0.0517\n",
            "Epoch 696/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0527 - val_loss: 0.0516\n",
            "Epoch 697/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0527 - val_loss: 0.0516\n",
            "Epoch 698/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0526 - val_loss: 0.0515\n",
            "Epoch 699/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0526 - val_loss: 0.0515\n",
            "Epoch 700/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0526 - val_loss: 0.0515\n",
            "Epoch 701/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0525 - val_loss: 0.0514\n",
            "Epoch 702/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0525 - val_loss: 0.0514\n",
            "Epoch 703/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0524 - val_loss: 0.0514\n",
            "Epoch 704/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0524 - val_loss: 0.0513\n",
            "Epoch 705/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0524 - val_loss: 0.0513\n",
            "Epoch 706/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0523 - val_loss: 0.0513\n",
            "Epoch 707/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0523 - val_loss: 0.0512\n",
            "Epoch 708/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0522 - val_loss: 0.0512\n",
            "Epoch 709/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0522 - val_loss: 0.0511\n",
            "Epoch 710/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0521 - val_loss: 0.0511\n",
            "Epoch 711/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0521 - val_loss: 0.0511\n",
            "Epoch 712/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0521 - val_loss: 0.0510\n",
            "Epoch 713/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0520 - val_loss: 0.0510\n",
            "Epoch 714/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0520 - val_loss: 0.0510\n",
            "Epoch 715/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0519 - val_loss: 0.0509\n",
            "Epoch 716/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0519 - val_loss: 0.0509\n",
            "Epoch 717/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0519 - val_loss: 0.0509\n",
            "Epoch 718/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0518 - val_loss: 0.0508\n",
            "Epoch 719/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0518 - val_loss: 0.0508\n",
            "Epoch 720/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0517 - val_loss: 0.0508\n",
            "Epoch 721/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0517 - val_loss: 0.0507\n",
            "Epoch 722/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0517 - val_loss: 0.0507\n",
            "Epoch 723/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0516 - val_loss: 0.0506\n",
            "Epoch 724/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0516 - val_loss: 0.0506\n",
            "Epoch 725/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0516 - val_loss: 0.0506\n",
            "Epoch 726/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0515 - val_loss: 0.0505\n",
            "Epoch 727/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0515 - val_loss: 0.0505\n",
            "Epoch 728/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0514 - val_loss: 0.0505\n",
            "Epoch 729/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0514 - val_loss: 0.0504\n",
            "Epoch 730/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0514 - val_loss: 0.0504\n",
            "Epoch 731/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0513 - val_loss: 0.0504\n",
            "Epoch 732/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0513 - val_loss: 0.0503\n",
            "Epoch 733/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0512 - val_loss: 0.0503\n",
            "Epoch 734/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0512 - val_loss: 0.0503\n",
            "Epoch 735/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0512 - val_loss: 0.0502\n",
            "Epoch 736/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0511 - val_loss: 0.0502\n",
            "Epoch 737/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0511 - val_loss: 0.0502\n",
            "Epoch 738/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0510 - val_loss: 0.0501\n",
            "Epoch 739/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0510 - val_loss: 0.0501\n",
            "Epoch 740/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0510 - val_loss: 0.0500\n",
            "Epoch 741/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0509 - val_loss: 0.0500\n",
            "Epoch 742/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0509 - val_loss: 0.0500\n",
            "Epoch 743/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0509 - val_loss: 0.0499\n",
            "Epoch 744/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0508 - val_loss: 0.0499\n",
            "Epoch 745/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0508 - val_loss: 0.0499\n",
            "Epoch 746/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0507 - val_loss: 0.0498\n",
            "Epoch 747/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0507 - val_loss: 0.0498\n",
            "Epoch 748/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0507 - val_loss: 0.0498\n",
            "Epoch 749/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0506 - val_loss: 0.0497\n",
            "Epoch 750/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0506 - val_loss: 0.0497\n",
            "Epoch 751/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0506 - val_loss: 0.0497\n",
            "Epoch 752/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0505 - val_loss: 0.0496\n",
            "Epoch 753/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0505 - val_loss: 0.0496\n",
            "Epoch 754/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0504 - val_loss: 0.0496\n",
            "Epoch 755/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0504 - val_loss: 0.0495\n",
            "Epoch 756/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0504 - val_loss: 0.0495\n",
            "Epoch 757/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0503 - val_loss: 0.0495\n",
            "Epoch 758/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0503 - val_loss: 0.0494\n",
            "Epoch 759/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0503 - val_loss: 0.0494\n",
            "Epoch 760/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0502 - val_loss: 0.0494\n",
            "Epoch 761/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0502 - val_loss: 0.0493\n",
            "Epoch 762/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0501 - val_loss: 0.0493\n",
            "Epoch 763/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0501 - val_loss: 0.0493\n",
            "Epoch 764/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0501 - val_loss: 0.0492\n",
            "Epoch 765/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0500 - val_loss: 0.0492\n",
            "Epoch 766/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0500 - val_loss: 0.0492\n",
            "Epoch 767/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0500 - val_loss: 0.0491\n",
            "Epoch 768/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0499 - val_loss: 0.0491\n",
            "Epoch 769/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0499 - val_loss: 0.0491\n",
            "Epoch 770/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0499 - val_loss: 0.0490\n",
            "Epoch 771/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0498 - val_loss: 0.0490\n",
            "Epoch 772/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0498 - val_loss: 0.0490\n",
            "Epoch 773/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0497 - val_loss: 0.0489\n",
            "Epoch 774/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0497 - val_loss: 0.0489\n",
            "Epoch 775/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0497 - val_loss: 0.0489\n",
            "Epoch 776/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0496 - val_loss: 0.0488\n",
            "Epoch 777/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0496 - val_loss: 0.0488\n",
            "Epoch 778/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0496 - val_loss: 0.0488\n",
            "Epoch 779/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0495 - val_loss: 0.0488\n",
            "Epoch 780/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0495 - val_loss: 0.0487\n",
            "Epoch 781/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0495 - val_loss: 0.0487\n",
            "Epoch 782/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0494 - val_loss: 0.0487\n",
            "Epoch 783/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0494 - val_loss: 0.0486\n",
            "Epoch 784/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0494 - val_loss: 0.0486\n",
            "Epoch 785/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0493 - val_loss: 0.0486\n",
            "Epoch 786/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0493 - val_loss: 0.0485\n",
            "Epoch 787/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0492 - val_loss: 0.0485\n",
            "Epoch 788/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0492 - val_loss: 0.0485\n",
            "Epoch 789/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0492 - val_loss: 0.0484\n",
            "Epoch 790/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0491 - val_loss: 0.0484\n",
            "Epoch 791/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0491 - val_loss: 0.0484\n",
            "Epoch 792/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0491 - val_loss: 0.0483\n",
            "Epoch 793/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0490 - val_loss: 0.0483\n",
            "Epoch 794/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0490 - val_loss: 0.0483\n",
            "Epoch 795/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0490 - val_loss: 0.0482\n",
            "Epoch 796/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0489 - val_loss: 0.0482\n",
            "Epoch 797/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0489 - val_loss: 0.0482\n",
            "Epoch 798/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0489 - val_loss: 0.0482\n",
            "Epoch 799/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0488 - val_loss: 0.0481\n",
            "Epoch 800/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0488 - val_loss: 0.0481\n",
            "Epoch 801/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0488 - val_loss: 0.0481\n",
            "Epoch 802/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0487 - val_loss: 0.0480\n",
            "Epoch 803/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0487 - val_loss: 0.0480\n",
            "Epoch 804/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0487 - val_loss: 0.0480\n",
            "Epoch 805/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0486 - val_loss: 0.0479\n",
            "Epoch 806/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0486 - val_loss: 0.0479\n",
            "Epoch 807/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0486 - val_loss: 0.0479\n",
            "Epoch 808/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0485 - val_loss: 0.0478\n",
            "Epoch 809/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0485 - val_loss: 0.0478\n",
            "Epoch 810/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0485 - val_loss: 0.0478\n",
            "Epoch 811/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0484 - val_loss: 0.0478\n",
            "Epoch 812/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0484 - val_loss: 0.0477\n",
            "Epoch 813/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0484 - val_loss: 0.0477\n",
            "Epoch 814/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0483 - val_loss: 0.0477\n",
            "Epoch 815/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0483 - val_loss: 0.0476\n",
            "Epoch 816/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0483 - val_loss: 0.0476\n",
            "Epoch 817/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0482 - val_loss: 0.0476\n",
            "Epoch 818/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0482 - val_loss: 0.0475\n",
            "Epoch 819/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0482 - val_loss: 0.0475\n",
            "Epoch 820/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0481 - val_loss: 0.0475\n",
            "Epoch 821/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0481 - val_loss: 0.0475\n",
            "Epoch 822/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0481 - val_loss: 0.0474\n",
            "Epoch 823/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0480 - val_loss: 0.0474\n",
            "Epoch 824/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0480 - val_loss: 0.0474\n",
            "Epoch 825/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0480 - val_loss: 0.0473\n",
            "Epoch 826/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0479 - val_loss: 0.0473\n",
            "Epoch 827/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0479 - val_loss: 0.0473\n",
            "Epoch 828/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0479 - val_loss: 0.0473\n",
            "Epoch 829/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0478 - val_loss: 0.0472\n",
            "Epoch 830/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0478 - val_loss: 0.0472\n",
            "Epoch 831/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0478 - val_loss: 0.0472\n",
            "Epoch 832/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0477 - val_loss: 0.0471\n",
            "Epoch 833/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0477 - val_loss: 0.0471\n",
            "Epoch 834/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0477 - val_loss: 0.0471\n",
            "Epoch 835/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0476 - val_loss: 0.0470\n",
            "Epoch 836/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0476 - val_loss: 0.0470\n",
            "Epoch 837/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0476 - val_loss: 0.0470\n",
            "Epoch 838/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0475 - val_loss: 0.0470\n",
            "Epoch 839/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0475 - val_loss: 0.0469\n",
            "Epoch 840/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0475 - val_loss: 0.0469\n",
            "Epoch 841/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0474 - val_loss: 0.0469\n",
            "Epoch 842/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0474 - val_loss: 0.0468\n",
            "Epoch 843/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0474 - val_loss: 0.0468\n",
            "Epoch 844/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0473 - val_loss: 0.0468\n",
            "Epoch 845/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0473 - val_loss: 0.0468\n",
            "Epoch 846/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0473 - val_loss: 0.0467\n",
            "Epoch 847/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0473 - val_loss: 0.0467\n",
            "Epoch 848/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0472 - val_loss: 0.0467\n",
            "Epoch 849/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0472 - val_loss: 0.0466\n",
            "Epoch 850/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0472 - val_loss: 0.0466\n",
            "Epoch 851/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0471 - val_loss: 0.0466\n",
            "Epoch 852/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0471 - val_loss: 0.0466\n",
            "Epoch 853/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0471 - val_loss: 0.0465\n",
            "Epoch 854/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0470 - val_loss: 0.0465\n",
            "Epoch 855/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0470 - val_loss: 0.0465\n",
            "Epoch 856/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0470 - val_loss: 0.0464\n",
            "Epoch 857/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0469 - val_loss: 0.0464\n",
            "Epoch 858/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0469 - val_loss: 0.0464\n",
            "Epoch 859/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0469 - val_loss: 0.0464\n",
            "Epoch 860/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0468 - val_loss: 0.0463\n",
            "Epoch 861/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0468 - val_loss: 0.0463\n",
            "Epoch 862/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0468 - val_loss: 0.0463\n",
            "Epoch 863/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0468 - val_loss: 0.0462\n",
            "Epoch 864/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0467 - val_loss: 0.0462\n",
            "Epoch 865/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0467 - val_loss: 0.0462\n",
            "Epoch 866/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0467 - val_loss: 0.0462\n",
            "Epoch 867/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0466 - val_loss: 0.0461\n",
            "Epoch 868/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0466 - val_loss: 0.0461\n",
            "Epoch 869/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0466 - val_loss: 0.0461\n",
            "Epoch 870/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0465 - val_loss: 0.0460\n",
            "Epoch 871/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0465 - val_loss: 0.0460\n",
            "Epoch 872/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0465 - val_loss: 0.0460\n",
            "Epoch 873/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0464 - val_loss: 0.0460\n",
            "Epoch 874/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0464 - val_loss: 0.0459\n",
            "Epoch 875/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0464 - val_loss: 0.0459\n",
            "Epoch 876/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0464 - val_loss: 0.0459\n",
            "Epoch 877/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0463 - val_loss: 0.0459\n",
            "Epoch 878/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0463 - val_loss: 0.0458\n",
            "Epoch 879/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0463 - val_loss: 0.0458\n",
            "Epoch 880/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0462 - val_loss: 0.0458\n",
            "Epoch 881/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0462 - val_loss: 0.0457\n",
            "Epoch 882/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0462 - val_loss: 0.0457\n",
            "Epoch 883/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0461 - val_loss: 0.0457\n",
            "Epoch 884/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0461 - val_loss: 0.0457\n",
            "Epoch 885/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0461 - val_loss: 0.0456\n",
            "Epoch 886/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0460 - val_loss: 0.0456\n",
            "Epoch 887/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0460 - val_loss: 0.0456\n",
            "Epoch 888/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0460 - val_loss: 0.0456\n",
            "Epoch 889/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0460 - val_loss: 0.0455\n",
            "Epoch 890/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0459 - val_loss: 0.0455\n",
            "Epoch 891/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0459 - val_loss: 0.0455\n",
            "Epoch 892/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0459 - val_loss: 0.0455\n",
            "Epoch 893/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0458 - val_loss: 0.0454\n",
            "Epoch 894/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0458 - val_loss: 0.0454\n",
            "Epoch 895/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0458 - val_loss: 0.0454\n",
            "Epoch 896/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0457 - val_loss: 0.0453\n",
            "Epoch 897/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0457 - val_loss: 0.0453\n",
            "Epoch 898/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0457 - val_loss: 0.0453\n",
            "Epoch 899/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0457 - val_loss: 0.0453\n",
            "Epoch 900/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0456 - val_loss: 0.0452\n",
            "Epoch 901/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0456 - val_loss: 0.0452\n",
            "Epoch 902/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0456 - val_loss: 0.0452\n",
            "Epoch 903/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0455 - val_loss: 0.0452\n",
            "Epoch 904/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0455 - val_loss: 0.0451\n",
            "Epoch 905/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0455 - val_loss: 0.0451\n",
            "Epoch 906/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0455 - val_loss: 0.0451\n",
            "Epoch 907/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0454 - val_loss: 0.0451\n",
            "Epoch 908/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0454 - val_loss: 0.0450\n",
            "Epoch 909/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0454 - val_loss: 0.0450\n",
            "Epoch 910/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0453 - val_loss: 0.0450\n",
            "Epoch 911/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0453 - val_loss: 0.0449\n",
            "Epoch 912/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0453 - val_loss: 0.0449\n",
            "Epoch 913/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0453 - val_loss: 0.0449\n",
            "Epoch 914/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0452 - val_loss: 0.0449\n",
            "Epoch 915/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0452 - val_loss: 0.0448\n",
            "Epoch 916/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0452 - val_loss: 0.0448\n",
            "Epoch 917/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0451 - val_loss: 0.0448\n",
            "Epoch 918/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0451 - val_loss: 0.0448\n",
            "Epoch 919/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0451 - val_loss: 0.0447\n",
            "Epoch 920/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0451 - val_loss: 0.0447\n",
            "Epoch 921/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0450 - val_loss: 0.0447\n",
            "Epoch 922/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0450 - val_loss: 0.0447\n",
            "Epoch 923/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0450 - val_loss: 0.0446\n",
            "Epoch 924/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0449 - val_loss: 0.0446\n",
            "Epoch 925/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0449 - val_loss: 0.0446\n",
            "Epoch 926/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0449 - val_loss: 0.0446\n",
            "Epoch 927/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0449 - val_loss: 0.0445\n",
            "Epoch 928/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0448 - val_loss: 0.0445\n",
            "Epoch 929/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0448 - val_loss: 0.0445\n",
            "Epoch 930/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0448 - val_loss: 0.0445\n",
            "Epoch 931/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0447 - val_loss: 0.0444\n",
            "Epoch 932/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0447 - val_loss: 0.0444\n",
            "Epoch 933/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0447 - val_loss: 0.0444\n",
            "Epoch 934/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0447 - val_loss: 0.0444\n",
            "Epoch 935/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0446 - val_loss: 0.0443\n",
            "Epoch 936/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0446 - val_loss: 0.0443\n",
            "Epoch 937/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0446 - val_loss: 0.0443\n",
            "Epoch 938/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0445 - val_loss: 0.0443\n",
            "Epoch 939/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0445 - val_loss: 0.0442\n",
            "Epoch 940/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0445 - val_loss: 0.0442\n",
            "Epoch 941/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0445 - val_loss: 0.0442\n",
            "Epoch 942/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0444 - val_loss: 0.0442\n",
            "Epoch 943/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0444 - val_loss: 0.0441\n",
            "Epoch 944/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0444 - val_loss: 0.0441\n",
            "Epoch 945/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0444 - val_loss: 0.0441\n",
            "Epoch 946/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0443 - val_loss: 0.0441\n",
            "Epoch 947/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0443 - val_loss: 0.0440\n",
            "Epoch 948/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0443 - val_loss: 0.0440\n",
            "Epoch 949/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0442 - val_loss: 0.0440\n",
            "Epoch 950/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0442 - val_loss: 0.0440\n",
            "Epoch 951/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0442 - val_loss: 0.0439\n",
            "Epoch 952/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0442 - val_loss: 0.0439\n",
            "Epoch 953/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0441 - val_loss: 0.0439\n",
            "Epoch 954/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0441 - val_loss: 0.0439\n",
            "Epoch 955/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0441 - val_loss: 0.0439\n",
            "Epoch 956/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0441 - val_loss: 0.0438\n",
            "Epoch 957/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0440 - val_loss: 0.0438\n",
            "Epoch 958/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0440 - val_loss: 0.0438\n",
            "Epoch 959/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0440 - val_loss: 0.0438\n",
            "Epoch 960/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0439 - val_loss: 0.0437\n",
            "Epoch 961/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0439 - val_loss: 0.0437\n",
            "Epoch 962/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0439 - val_loss: 0.0437\n",
            "Epoch 963/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0439 - val_loss: 0.0437\n",
            "Epoch 964/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0438 - val_loss: 0.0436\n",
            "Epoch 965/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0438 - val_loss: 0.0436\n",
            "Epoch 966/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0438 - val_loss: 0.0436\n",
            "Epoch 967/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0438 - val_loss: 0.0436\n",
            "Epoch 968/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0437 - val_loss: 0.0435\n",
            "Epoch 969/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0437 - val_loss: 0.0435\n",
            "Epoch 970/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0437 - val_loss: 0.0435\n",
            "Epoch 971/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0436 - val_loss: 0.0435\n",
            "Epoch 972/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0436 - val_loss: 0.0434\n",
            "Epoch 973/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0436 - val_loss: 0.0434\n",
            "Epoch 974/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0436 - val_loss: 0.0434\n",
            "Epoch 975/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0435 - val_loss: 0.0434\n",
            "Epoch 976/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0435 - val_loss: 0.0434\n",
            "Epoch 977/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0435 - val_loss: 0.0433\n",
            "Epoch 978/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0435 - val_loss: 0.0433\n",
            "Epoch 979/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0434 - val_loss: 0.0433\n",
            "Epoch 980/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0434 - val_loss: 0.0433\n",
            "Epoch 981/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0434 - val_loss: 0.0432\n",
            "Epoch 982/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0434 - val_loss: 0.0432\n",
            "Epoch 983/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0433 - val_loss: 0.0432\n",
            "Epoch 984/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0433 - val_loss: 0.0432\n",
            "Epoch 985/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0433 - val_loss: 0.0431\n",
            "Epoch 986/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0433 - val_loss: 0.0431\n",
            "Epoch 987/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0432 - val_loss: 0.0431\n",
            "Epoch 988/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0432 - val_loss: 0.0431\n",
            "Epoch 989/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0432 - val_loss: 0.0430\n",
            "Epoch 990/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0431 - val_loss: 0.0430\n",
            "Epoch 991/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0431 - val_loss: 0.0430\n",
            "Epoch 992/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0431 - val_loss: 0.0430\n",
            "Epoch 993/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0431 - val_loss: 0.0430\n",
            "Epoch 994/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0430 - val_loss: 0.0429\n",
            "Epoch 995/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0430 - val_loss: 0.0429\n",
            "Epoch 996/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0430 - val_loss: 0.0429\n",
            "Epoch 997/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0430 - val_loss: 0.0429\n",
            "Epoch 998/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0429 - val_loss: 0.0428\n",
            "Epoch 999/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0429 - val_loss: 0.0428\n",
            "Epoch 1000/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0429 - val_loss: 0.0428\n",
            "Epoch 1001/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0429 - val_loss: 0.0428\n",
            "Epoch 1002/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0428 - val_loss: 0.0427\n",
            "Epoch 1003/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0428 - val_loss: 0.0427\n",
            "Epoch 1004/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0428 - val_loss: 0.0427\n",
            "Epoch 1005/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0428 - val_loss: 0.0427\n",
            "Epoch 1006/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0427 - val_loss: 0.0427\n",
            "Epoch 1007/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0427 - val_loss: 0.0426\n",
            "Epoch 1008/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0427 - val_loss: 0.0426\n",
            "Epoch 1009/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0427 - val_loss: 0.0426\n",
            "Epoch 1010/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0426 - val_loss: 0.0426\n",
            "Epoch 1011/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0426 - val_loss: 0.0425\n",
            "Epoch 1012/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0426 - val_loss: 0.0425\n",
            "Epoch 1013/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0426 - val_loss: 0.0425\n",
            "Epoch 1014/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0425 - val_loss: 0.0425\n",
            "Epoch 1015/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0425 - val_loss: 0.0425\n",
            "Epoch 1016/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0425 - val_loss: 0.0424\n",
            "Epoch 1017/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0425 - val_loss: 0.0424\n",
            "Epoch 1018/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0424 - val_loss: 0.0424\n",
            "Epoch 1019/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0424 - val_loss: 0.0424\n",
            "Epoch 1020/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0424 - val_loss: 0.0423\n",
            "Epoch 1021/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0424 - val_loss: 0.0423\n",
            "Epoch 1022/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0423 - val_loss: 0.0423\n",
            "Epoch 1023/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0423 - val_loss: 0.0423\n",
            "Epoch 1024/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0423 - val_loss: 0.0423\n",
            "Epoch 1025/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0423 - val_loss: 0.0422\n",
            "Epoch 1026/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0422 - val_loss: 0.0422\n",
            "Epoch 1027/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0422 - val_loss: 0.0422\n",
            "Epoch 1028/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0422 - val_loss: 0.0422\n",
            "Epoch 1029/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0422 - val_loss: 0.0421\n",
            "Epoch 1030/2000\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 0.0421 - val_loss: 0.0421\n",
            "Epoch 1031/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0421 - val_loss: 0.0421\n",
            "Epoch 1032/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0421 - val_loss: 0.0421\n",
            "Epoch 1033/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0421 - val_loss: 0.0421\n",
            "Epoch 1034/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0420 - val_loss: 0.0420\n",
            "Epoch 1035/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0420 - val_loss: 0.0420\n",
            "Epoch 1036/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0420 - val_loss: 0.0420\n",
            "Epoch 1037/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0420 - val_loss: 0.0420\n",
            "Epoch 1038/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0419 - val_loss: 0.0419\n",
            "Epoch 1039/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0419 - val_loss: 0.0419\n",
            "Epoch 1040/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0419 - val_loss: 0.0419\n",
            "Epoch 1041/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0419 - val_loss: 0.0419\n",
            "Epoch 1042/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0418 - val_loss: 0.0419\n",
            "Epoch 1043/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0418 - val_loss: 0.0418\n",
            "Epoch 1044/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0418 - val_loss: 0.0418\n",
            "Epoch 1045/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0418 - val_loss: 0.0418\n",
            "Epoch 1046/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0417 - val_loss: 0.0418\n",
            "Epoch 1047/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0417 - val_loss: 0.0417\n",
            "Epoch 1048/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0417 - val_loss: 0.0417\n",
            "Epoch 1049/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0417 - val_loss: 0.0417\n",
            "Epoch 1050/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0416 - val_loss: 0.0417\n",
            "Epoch 1051/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0416 - val_loss: 0.0417\n",
            "Epoch 1052/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0416 - val_loss: 0.0416\n",
            "Epoch 1053/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0416 - val_loss: 0.0416\n",
            "Epoch 1054/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0415 - val_loss: 0.0416\n",
            "Epoch 1055/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0415 - val_loss: 0.0416\n",
            "Epoch 1056/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0415 - val_loss: 0.0416\n",
            "Epoch 1057/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0415 - val_loss: 0.0415\n",
            "Epoch 1058/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0414 - val_loss: 0.0415\n",
            "Epoch 1059/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0414 - val_loss: 0.0415\n",
            "Epoch 1060/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0414 - val_loss: 0.0415\n",
            "Epoch 1061/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0414 - val_loss: 0.0414\n",
            "Epoch 1062/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0414 - val_loss: 0.0414\n",
            "Epoch 1063/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0413 - val_loss: 0.0414\n",
            "Epoch 1064/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0413 - val_loss: 0.0414\n",
            "Epoch 1065/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0413 - val_loss: 0.0414\n",
            "Epoch 1066/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0413 - val_loss: 0.0413\n",
            "Epoch 1067/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0412 - val_loss: 0.0413\n",
            "Epoch 1068/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0412 - val_loss: 0.0413\n",
            "Epoch 1069/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0412 - val_loss: 0.0413\n",
            "Epoch 1070/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0412 - val_loss: 0.0413\n",
            "Epoch 1071/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0411 - val_loss: 0.0412\n",
            "Epoch 1072/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0411 - val_loss: 0.0412\n",
            "Epoch 1073/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0411 - val_loss: 0.0412\n",
            "Epoch 1074/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0411 - val_loss: 0.0412\n",
            "Epoch 1075/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0410 - val_loss: 0.0411\n",
            "Epoch 1076/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0410 - val_loss: 0.0411\n",
            "Epoch 1077/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0410 - val_loss: 0.0411\n",
            "Epoch 1078/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0410 - val_loss: 0.0411\n",
            "Epoch 1079/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0410 - val_loss: 0.0411\n",
            "Epoch 1080/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0409 - val_loss: 0.0410\n",
            "Epoch 1081/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0409 - val_loss: 0.0410\n",
            "Epoch 1082/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0409 - val_loss: 0.0410\n",
            "Epoch 1083/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0409 - val_loss: 0.0410\n",
            "Epoch 1084/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0408 - val_loss: 0.0410\n",
            "Epoch 1085/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0408 - val_loss: 0.0409\n",
            "Epoch 1086/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0408 - val_loss: 0.0409\n",
            "Epoch 1087/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0408 - val_loss: 0.0409\n",
            "Epoch 1088/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0407 - val_loss: 0.0409\n",
            "Epoch 1089/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0407 - val_loss: 0.0409\n",
            "Epoch 1090/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0407 - val_loss: 0.0408\n",
            "Epoch 1091/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0407 - val_loss: 0.0408\n",
            "Epoch 1092/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0407 - val_loss: 0.0408\n",
            "Epoch 1093/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0406 - val_loss: 0.0408\n",
            "Epoch 1094/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0406 - val_loss: 0.0408\n",
            "Epoch 1095/2000\n",
            "600/600 [==============================] - 3s 6ms/step - loss: 0.0406 - val_loss: 0.0407\n",
            "Epoch 1096/2000\n",
            "600/600 [==============================] - 4s 6ms/step - loss: 0.0406 - val_loss: 0.0407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = numberGuesser.predict(x_test)"
      ],
      "metadata": {
        "id": "VEtD2V-v1vwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl.plot(np.array(numberFitter.history['loss']), label=\"training\")\n",
        "pl.plot(np.array(numberFitter.history['val_loss']), label=\"validation\")\n",
        "pl.ylabel('loss')\n",
        "pl.xlabel('iteration')\n",
        "pl.legend()\n",
        "#pl.yscale('log')\n",
        "#pl.xscale('log')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "txCUh2oy4n8J",
        "outputId": "ec95b5fd-4054-490b-9148-be864b3007cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f48b61b3550>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_YijMGC3Aio",
        "outputId": "fc5228fb-dd78-4d6d-9d1d-5cd014a75e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.1512322e-07, 9.8094821e-01, 3.3094941e-03, 2.8305869e-03,\n",
              "       2.4105543e-03, 6.5003056e-05, 9.1299030e-04, 8.7106545e-03,\n",
              "       2.7309963e-03, 3.4152321e-04], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compareLabelandPred(i, testimg, real, pred):\n",
        "  #print(\"Real #: \", real[i], \"\\tPredicted #: \", pred)\n",
        "  val = real[i] - pred\n",
        "  if(val != 0):\n",
        "    print(\"We guessed:\", pred, \"but it was really \", real[i])\n",
        "    #fig = pl.figure(figsize(10,5))\n",
        "    #ax = fig.add_subplot(111)\n",
        "    #ax.imshow(testimg[i].reshape((28,28)), cmap=\"bone\")\n",
        "    #ax.text(1.2, 1.2, \"Predicted Number: \" + str(pred) + \"  Real Number: \" + str(real[i]), bbox={'facecolor': 'white', 'pad': 10})\n",
        "  return 1 if val == 0 else 0"
      ],
      "metadata": {
        "id": "jcmsNbop9oV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compareinandnum(i, testimg, predictedNum, initialshape=(28,28)):\n",
        "  fig = pl.figure(figsize(10,5))\n",
        "  ax = fig.add_subplot(111) \n",
        "  ax.imshow(testimg[i].reshape(initialshape) , cmap=\"bone\")\n",
        "  #ax.get_xaxis().set_visible(False)\n",
        "  #ax.get_yaxis().set_visible(False)\n",
        "  #ax.label = \"Predicted Number: \" + str(predictedNum)\n",
        "  print(\"Predicted Number: \", predictedNum)\n",
        "  #ax.legend()\n"
      ],
      "metadata": {
        "id": "03UV-rvM7ule"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J8kWH2-3CO1",
        "outputId": "498ec6b0-a13f-4517-93d1-c92bbf44216b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.41956878"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "argmax(output[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjSyTNsw7sUY",
        "outputId": "b5f06439-f641-4867-cd22-9a45d1e2e875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numRight = 0\n",
        "n = 10000\n",
        "for i in range(n):\n",
        "  ret = compareLabelandPred(i, x_test, l_test, argmax(output[i]))\n",
        "  numRight = numRight + ret\n",
        "print(\"We got \", numRight, \" out of \", n)\n",
        "percCorrect = float(numRight) / n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFlOzxZl8amq",
        "outputId": "8866831f-14cf-4a28-e0a1-34ab7772f1a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We guessed: 6 but it was really  5\n",
            "We guessed: 6 but it was really  4\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 4 but it was really  7\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 8 but it was really  9\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 7 but it was really  8\n",
            "We guessed: 8 but it was really  9\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 2 but it was really  4\n",
            "We guessed: 0 but it was really  6\n",
            "We guessed: 1 but it was really  4\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 7 but it was really  9\n",
            "We guessed: 7 but it was really  2\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 0 but it was really  5\n",
            "We guessed: 7 but it was really  2\n",
            "We guessed: 7 but it was really  3\n",
            "We guessed: 7 but it was really  8\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 0 but it was really  6\n",
            "We guessed: 8 but it was really  9\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 3 but it was really  9\n",
            "We guessed: 0 but it was really  8\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 7 but it was really  4\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 1 but it was really  7\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 2 but it was really  8\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 4 but it was really  8\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 6 but it was really  0\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 9 but it was really  5\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 7 but it was really  8\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 8 but it was really  7\n",
            "We guessed: 7 but it was really  2\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 0 but it was really  2\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 4 but it was really  5\n",
            "We guessed: 2 but it was really  1\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 0 but it was really  6\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 2 but it was really  6\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 8 but it was really  6\n",
            "We guessed: 7 but it was really  3\n",
            "We guessed: 4 but it was really  8\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 2 but it was really  8\n",
            "We guessed: 2 but it was really  8\n",
            "We guessed: 3 but it was really  9\n",
            "We guessed: 6 but it was really  4\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 1 but it was really  6\n",
            "We guessed: 6 but it was really  0\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 4 but it was really  8\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 3 but it was really  9\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 5 but it was really  8\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 0 but it was really  9\n",
            "We guessed: 0 but it was really  2\n",
            "We guessed: 1 but it was really  7\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 9 but it was really  5\n",
            "We guessed: 7 but it was really  5\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 1 but it was really  7\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 6 but it was really  2\n",
            "We guessed: 2 but it was really  8\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 6 but it was really  2\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 9 but it was really  6\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 0 but it was really  7\n",
            "We guessed: 1 but it was really  7\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 0 but it was really  5\n",
            "We guessed: 6 but it was really  1\n",
            "We guessed: 7 but it was really  8\n",
            "We guessed: 6 but it was really  4\n",
            "We guessed: 3 but it was really  9\n",
            "We guessed: 2 but it was really  6\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 6 but it was really  0\n",
            "We guessed: 7 but it was really  4\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 0 but it was really  2\n",
            "We guessed: 7 but it was really  3\n",
            "We guessed: 5 but it was really  9\n",
            "We guessed: 0 but it was really  8\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 2 but it was really  5\n",
            "We guessed: 2 but it was really  4\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 4 but it was really  7\n",
            "We guessed: 5 but it was really  8\n",
            "We guessed: 7 but it was really  2\n",
            "We guessed: 5 but it was really  8\n",
            "We guessed: 7 but it was really  8\n",
            "We guessed: 4 but it was really  6\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 0 but it was really  6\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 6 but it was really  4\n",
            "We guessed: 0 but it was really  5\n",
            "We guessed: 5 but it was really  9\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 0 but it was really  2\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 8 but it was really  4\n",
            "We guessed: 7 but it was really  2\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 0 but it was really  2\n",
            "We guessed: 7 but it was really  3\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 2 but it was really  9\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 1 but it was really  6\n",
            "We guessed: 2 but it was really  1\n",
            "We guessed: 5 but it was really  0\n",
            "We guessed: 0 but it was really  2\n",
            "We guessed: 1 but it was really  9\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 6 but it was really  1\n",
            "We guessed: 0 but it was really  8\n",
            "We guessed: 6 but it was really  9\n",
            "We guessed: 7 but it was really  2\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 1 but it was really  9\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 1 but it was really  9\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 4 but it was really  6\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 9 but it was really  5\n",
            "We guessed: 2 but it was really  8\n",
            "We guessed: 8 but it was really  7\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 7 but it was really  2\n",
            "We guessed: 0 but it was really  9\n",
            "We guessed: 1 but it was really  6\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 4 but it was really  7\n",
            "We guessed: 8 but it was really  0\n",
            "We guessed: 4 but it was really  7\n",
            "We guessed: 0 but it was really  6\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 3 but it was really  2\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 4 but it was really  6\n",
            "We guessed: 0 but it was really  8\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 0 but it was really  5\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 5 but it was really  9\n",
            "We guessed: 7 but it was really  3\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 7 but it was really  5\n",
            "We guessed: 8 but it was really  6\n",
            "We guessed: 1 but it was really  9\n",
            "We guessed: 7 but it was really  9\n",
            "We guessed: 2 but it was really  1\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 6 but it was really  4\n",
            "We guessed: 9 but it was really  5\n",
            "We guessed: 0 but it was really  6\n",
            "We guessed: 2 but it was really  1\n",
            "We guessed: 8 but it was really  7\n",
            "We guessed: 9 but it was really  5\n",
            "We guessed: 7 but it was really  5\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 1 but it was really  3\n",
            "We guessed: 4 but it was really  7\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 3 but it was really  9\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 9 but it was really  8\n",
            "We guessed: 4 but it was really  7\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 3 but it was really  2\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 8 but it was really  0\n",
            "We guessed: 7 but it was really  3\n",
            "We guessed: 3 but it was really  4\n",
            "We guessed: 1 but it was really  9\n",
            "We guessed: 4 but it was really  6\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 0 but it was really  5\n",
            "We guessed: 5 but it was really  8\n",
            "We guessed: 4 but it was really  7\n",
            "We guessed: 3 but it was really  9\n",
            "We guessed: 0 but it was really  7\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 7 but it was really  5\n",
            "We guessed: 5 but it was really  8\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 3 but it was really  9\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 0 but it was really  6\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 8 but it was really  9\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 4 but it was really  5\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 0 but it was really  5\n",
            "We guessed: 6 but it was really  4\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 0 but it was really  6\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 3 but it was really  7\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 6 but it was really  0\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 4 but it was really  7\n",
            "We guessed: 1 but it was really  7\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 3 but it was really  7\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 0 but it was really  5\n",
            "We guessed: 3 but it was really  2\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 3 but it was really  1\n",
            "We guessed: 3 but it was really  9\n",
            "We guessed: 2 but it was really  4\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 1 but it was really  7\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 2 but it was really  0\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 0 but it was really  8\n",
            "We guessed: 7 but it was really  9\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 1 but it was really  5\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 0 but it was really  9\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 4 but it was really  1\n",
            "We guessed: 6 but it was really  2\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 3 but it was really  1\n",
            "We guessed: 7 but it was really  9\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 8 but it was really  4\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 7 but it was really  2\n",
            "We guessed: 9 but it was really  5\n",
            "We guessed: 7 but it was really  3\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 9 but it was really  5\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 5 but it was really  8\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 1 but it was really  7\n",
            "We guessed: 7 but it was really  3\n",
            "We guessed: 0 but it was really  6\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 7 but it was really  9\n",
            "We guessed: 7 but it was really  8\n",
            "We guessed: 8 but it was really  7\n",
            "We guessed: 1 but it was really  9\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 2 but it was really  4\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 4 but it was really  8\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 9 but it was really  8\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 7 but it was really  8\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 6 but it was really  4\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 0 but it was really  8\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 0 but it was really  6\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 0 but it was really  9\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 8 but it was really  0\n",
            "We guessed: 1 but it was really  7\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 3 but it was really  2\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 4 but it was really  8\n",
            "We guessed: 1 but it was really  7\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 1 but it was really  8\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 1 but it was really  4\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 5 but it was really  0\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 6 but it was really  1\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 6 but it was really  0\n",
            "We guessed: 5 but it was really  8\n",
            "We guessed: 1 but it was really  4\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 1 but it was really  4\n",
            "We guessed: 7 but it was really  3\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 9 but it was really  7\n",
            "We guessed: 7 but it was really  4\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 0 but it was really  7\n",
            "We guessed: 0 but it was really  4\n",
            "We guessed: 0 but it was really  3\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 0 but it was really  2\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 3 but it was really  9\n",
            "We guessed: 5 but it was really  9\n",
            "We guessed: 0 but it was really  9\n",
            "We guessed: 0 but it was really  9\n",
            "We guessed: 0 but it was really  9\n",
            "We guessed: 3 but it was really  9\n",
            "We guessed: 3 but it was really  9\n",
            "We guessed: 0 but it was really  9\n",
            "We guessed: 8 but it was really  9\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 2 but it was really  5\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 6 but it was really  0\n",
            "We guessed: 6 but it was really  2\n",
            "We guessed: 0 but it was really  9\n",
            "We guessed: 9 but it was really  8\n",
            "We guessed: 9 but it was really  3\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 6 but it was really  2\n",
            "We guessed: 1 but it was really  7\n",
            "We guessed: 7 but it was really  0\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 7 but it was really  8\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 7 but it was really  8\n",
            "We guessed: 5 but it was really  9\n",
            "We guessed: 5 but it was really  8\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 5 but it was really  0\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 0 but it was really  9\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 4 but it was really  5\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 1 but it was really  5\n",
            "We guessed: 6 but it was really  1\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 4 but it was really  6\n",
            "We guessed: 2 but it was really  4\n",
            "We guessed: 4 but it was really  6\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 1 but it was really  7\n",
            "We guessed: 8 but it was really  4\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 9 but it was really  2\n",
            "We guessed: 7 but it was really  2\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 9 but it was really  3\n",
            "We guessed: 0 but it was really  6\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 4 but it was really  5\n",
            "We guessed: 0 but it was really  6\n",
            "We guessed: 4 but it was really  5\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 4 but it was really  5\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 4 but it was really  2\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 8 but it was really  5\n",
            "We guessed: 6 but it was really  4\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 8 but it was really  4\n",
            "We guessed: 5 but it was really  8\n",
            "We guessed: 9 but it was really  3\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 4 but it was really  8\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 3 but it was really  2\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 5 but it was really  0\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 2 but it was really  3\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 2 but it was really  7\n",
            "We guessed: 8 but it was really  1\n",
            "We guessed: 9 but it was really  4\n",
            "We guessed: 5 but it was really  8\n",
            "We guessed: 3 but it was really  8\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 7 but it was really  9\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 5 but it was really  3\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 3 but it was really  0\n",
            "We guessed: 7 but it was really  9\n",
            "We guessed: 7 but it was really  1\n",
            "We guessed: 7 but it was really  2\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 7 but it was really  9\n",
            "We guessed: 5 but it was really  6\n",
            "We guessed: 6 but it was really  2\n",
            "We guessed: 0 but it was really  2\n",
            "We guessed: 0 but it was really  5\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 1 but it was really  8\n",
            "We guessed: 2 but it was really  4\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 0 but it was really  2\n",
            "We guessed: 0 but it was really  2\n",
            "We guessed: 8 but it was really  4\n",
            "We guessed: 0 but it was really  2\n",
            "We guessed: 0 but it was really  5\n",
            "We guessed: 0 but it was really  5\n",
            "We guessed: 0 but it was really  2\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 7 but it was really  2\n",
            "We guessed: 8 but it was really  6\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 6 but it was really  0\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 8 but it was really  6\n",
            "We guessed: 6 but it was really  8\n",
            "We guessed: 8 but it was really  2\n",
            "We guessed: 4 but it was really  9\n",
            "We guessed: 7 but it was really  3\n",
            "We guessed: 6 but it was really  5\n",
            "We guessed: 8 but it was really  3\n",
            "We guessed: 3 but it was really  5\n",
            "We guessed: 3 but it was really  2\n",
            "We guessed: 6 but it was really  5\n",
            "We got  9405  out of  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(percCorrect)"
      ],
      "metadata": {
        "id": "43drTBgU8mE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "042193a6-0aa2-46b8-92d4-c24e02883400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHA4zE6xmacQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "52f23ab4-7b2c-476a-8e20-a4e7b879390b"
      },
      "source": [
        "from keras import backend as K\n",
        "# input placeholder\n",
        "inp = model_digits.input                   \n",
        "# extract the bottle neck outputs\n",
        "outputs = model_digits.layers[3].output     \n",
        "# create a function to evaluate the output of the bottle neck layer for a given input\n",
        "functors = K.function([inp], [outputs])    \n",
        "\n",
        "# Testing\n",
        "layer_outs = functors(x_test[:1])\n",
        "pl.imshow(layer_outs[0]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}